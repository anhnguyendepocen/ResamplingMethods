---
title: "Resampling Methods"
author: "Clay Ford"
date: "November 2014"
output: beamer_presentation

---

## Traditional Statistical Inference

To estimate the mean of a population:

- Take a random sample from the population
- Calculate the mean of the sample to estimate population mean
- Assess variability of sample mean by calculating standard error: $\frac{s}{\sqrt{n}}$, where $s$ is the standard deviation of the sample
          
The standard error gives us an idea of how accurate our estimate is. 

Two items of note about the standard error:

1. the concise formula
2. it assumes the distribution of the mean is normal

## Illustration of Sampling Distribution of mean


```{r, echo=FALSE}
# "draw" a population
par(mfrow=c(2,1))
x <- seq(0,20,0.01)
y <- dchisq(x,5)
plot(x,y,type="l", main="A Skewed Population with mean=5", xlim=c(0,20))
segments(x0 = 5,y0 = 0,x1 = 5,y1 = dchisq(5,5)) # add mean
# add arrow to show 1 standard deviation
# arrows(25,dnorm(25+5,25,5), (25+5), dnorm(25+5,25,5),col="red")

# build a sampling distribution via simulation;
# sample n=15 from the population and find mean; 
# repeat 5000 times and create histogram to visualize sampling distribution;
set.seed(123)
sample.means <- replicate(10000, mean(rchisq(15, 5)))
hist(sample.means, freq=F, xlim=c(0,20),
     breaks=30, main="Simulated Sampling Distribution of mean for n=15",
     sub="red arrow is standard deviation of sampling distribution; the standard error estimates it")
sem <- sd(sample.means) # estimated standard error
# add arrow to show 1 standard deviation (ie, 1 standard error)
lines(x,dnorm(x,5,sem))
arrows(5,dnorm(5+sem,5,sem), (5+sem), 
       dnorm(5+sem,5,sem),col="red")
box()
```


## What about other statistics?

For example, median, trimmed mean, ratio, correlation, standard deviation, error rate, ...

Sample means are essentially the only statistic with an easy-to-obtain standard error formula.

Deriving standard error formulas for other statistics can be tedious or even impossible.

Derivation of standard error formulas also requires making (possibly incorrect) assumptions about the shape of the sampling distribution.


## Sampling Distribution of Median

```{r, echo=FALSE}
par(mfrow=c(2,1))
set.seed(2112)
N <- 10000
probs <- 0.5
dists <- runif(N)  
# mixture of chisq and normal
data1 <- vector(length=N)
for(i in 1:N){
  if(dists[i]<probs){
    data1[i] = rchisq(1,4)
  } else {
    data1[i] = rnorm(1, 20, 10)
  }
}
outd <- density(data1)
plot(outd,xlab="",main="A skewed population")
yy <- which(abs(outd$x - median(data1))==min(abs(outd$x - median(data1))))
segments(x0 = median(data1), y0 = 0, x1 = median(data1), y1 = outd$y[yy])

outm <- replicate(10000,median(sample(data1,15)))
hist(outm,breaks=30,freq=F,xlim=c(-20,60),ylim=c(0,0.15), 
     xlab="", main="Simulated Sampling Distribution of median for n=15",
     sub="Sampling distribution is not normally distributed. How do we calculate standard error?")
sem <- sd(outm)
x <- seq(-10,30,length.out = 200)
lines(x,dnorm(x,mean(outm),sem),col="blue")
box()
par(mfrow=c(1,1))
```


## Introducing Resampling Methods

Replace complicated or inaccurate approximations with _computer simulations_. Also known as "Bootstrapping Methods".

- Use computer to resample from _original random sample_ to create replicate datasets (bootstrap samples)
- Calculate statistic of interest for _each_ replicate dataset (bootstrap replications)
- Summarize bootstrap replications (example: take standard deviation to estimate standard error)
    
Resampling methods can assess other accuracy measures such as bias, prediction errors, and confidence intervals.


## Example of Bootstrap Sample

1. resample _with replacement_
2. resample the same amount as original data

Example using R:
```{r}
# original sample data (n=10)
myData <- c(18,18,29,20,11,12,16,25,24,21)
# resample data (n=10)
sample(myData, replace=TRUE)
```


## Example of Bootstrap Replication

Calculate statistic of interest for each bootstrap sample.

          
```{r}
# median of original sample
median(myData)
# median of resample
median(sample(myData, replace=TRUE))

```

We want to do this at least 200 times; in practice we use 999 or more.


## Example of 200 Bootstrap Replications

```{r}
# resample data 200 times, take median each time
bout <- replicate(200, 
                  median(sample(myData, replace=T)))
# bootstrap estimate of standard error:
sd(bout)

```

Note we'll get a slightly different answer each time we do this.

This was a toy example. We'll use the `boot` package in R for formal resampling procedures.

## Sampling Distribution vs. Bootstrap Distribution

```{r, echo=FALSE}
par(mfrow=c(1,2))
set.seed(123)
sample.means <- replicate(1000, mean(rnorm(15, 25, 5)))
hist(sample.means, xlim=c(20,30), ylim=c(0,0.35), freq=F, breaks=20, ,xlab="", ylab="",
     main="Simulated Sampling Distribution\n of mean from N(25,5)\n n=15")
xx <- seq(20,30,length.out = 200)
lines(xx,dnorm(xx,25,(5/sqrt(15))),col="blue")
x <- rnorm(15,25,5)
library(boot)
boot.means <- boot(x,function(x,i)mean(x[i]),R=1000)
# boot.means <- replicate(1000, mean(sample(x, replace=T)))
hist(boot.means$t, xlim=c(20,30), ylim=c(0,0.35), freq=F, breaks=20, xlab="", ylab="",
     main="Bootstrap Distribution\n of mean using one sample of n=15 \n from N(25,5)")
lines(xx,dnorm(xx,25,(5/sqrt(15))),col="blue")

```


## Limitations of the Bootstrap

The bootstrap requires that the sample serve as a surrogate for the population.

The bootstrap **will fail** if:

- the original sample is not random 
- the original sample is biased
- the sample objects are not independent of one another

Again, the original sample must be representative of the population!

## Bootstrap Algorithm for Estimating Standard Errors

1. Select B independent bootstrap samples $\textbf{x}^{*1},\textbf{x}^{*2},\cdots,\textbf{x}^{*B}$ each consisting of _n_ data values drawn with replacement from **the original data set**
2. Evaluate the bootstrap replication corresponding to each bootstrap sample,
$$ \hat{\theta}^{*}(b) = s(\textbf{x}^{*b}) \hspace{10 mm} b = 1,2,\cdots,B$$
3. Estimate the standard error by the sample standard deviation of the B replications:
$$\hat{se}_{B}=\sqrt{\sum_{b=1}^{B}[\hat{\theta}^{*}(b) - \hat{\theta}^{*}(\cdot)]^{2}/(B - 1)}$$

where $\hat{\theta}^{*}(\cdot) = \sum_{b=1}^{B}\hat{\theta}^{*}(b)/B$

## Implementing Bootstrap Algorithms in R
Use the `boot` package.

- Comes with base R installation.
- Returns a convenient `boot` object with associated `print` and `plot` methods
- preserves bootstrap replicates
- allows easy calculation of confidence intervals
- implements variants on the bootstrap (stratified sampling, time series, censored data)

One catch: user must supply a function.

Let's go to R.

## Bias

Another useful measure of statistical accuracy is _Bias_.

Bias is the difference between the expectation of an estimator $\hat{\theta}$ and the quantity $\theta$ being estimated.

Large bias is almost always undesirable.

![bias](images/bias2.jpg)


## Bootstrap Estimate of Bias

The bootstrap estimate of bias is

$$\widehat{bias}_{B} = \hat{\theta}^{*}(\cdot) - \hat{\theta}$$

where $\hat{\theta}^{*}(\cdot) = \sum_{b=1}^{B}\hat{\theta}^{*}(b)/B$

In other words, take the mean of the bootstrap replications and subtract the original estimate.

As a rule of thumb, bias less than 0.25 standard errors can be ignored. ($|\widehat{bias}/\widehat{se}| < 0.25$)

Large bias may be an indication that your statistic is not an appropriate estimator of the population parameter.

Let's go to R.

## The Jackknife

The jackknife, first proposed in the 1950's, is the original computer-based method for estimating biases and standard errors.

The jackknife focuses on the samples that _leave out one observation at a time_:

$$\textbf{x}_{(i)} = (x_{1}, x_{2},...x_{i-1}, x_{i+1},...x_{n})$$

The _i_th jackknife sample consists of the data set with the _i_th observation removed. For example, $\textbf{x}_{(1)}$ is the data set with the first observation removed. 

$\hat{\theta}_{(i)}$ is the _i_th jackknife replication of $\hat{\theta}$. $\hat{\theta}_{(1)}$ is the jackknife estimate with the first observation removed.

## The Jackknife Estimates of Bias and Standard Error

The jackknife estimate of bias:

$$\widehat{bias}_{jack} = (n-1)(\hat{\theta}_{(\cdot)} - \hat{\theta})$$

where $\hat{\theta}_{(\cdot)} = \sum_{i=1}^{n}\hat{\theta}_{(i)}/n$.

The jackknife estimate of standard error:

$$\widehat{se}_{jack} = \sqrt{ \frac{n-1}{n}\sum(\hat{\theta}_{(i)} - \hat{\theta}_{(\cdot)})^{2}}$$

The jackknife provides a simple approximation to the bootstrap for estimation of bias and standard errors.

Let's go to R.

## The Jackknife-after-Bootstrap Plot

A common use of the jackknife is for bootstrap diagnostics.

The jackknife-after-bootstrap plot looks at bootstrap samples in which the _i_th point did not appear.

The plot shows the sensitivity of the statistic and of the percentiles of its bootstrapped distribution to deletion of individual observations.

Centered quantiles are generated for all jackknife samples and plotted against _standardized jackknife influence values_:

$$l_{jack,j} = (n - 1)(\hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)})/\sqrt{(var(\hat{\theta}_{(i)})} $$

R Syntax: `jack.after.boot(boot.object)` or `plot(boot.object, jack=TRUE) `

## The Jackknife-after-Bootstrap Plot - Example
```{r, echo=FALSE}
library(bootstrap)
library(boot)
ratio.fun <- function(dat, ind){
  tmp <- dat[ind,]
  mean(tmp$y)/mean(tmp$z)
}

# do the bootstrap
bout <- boot(patch, ratio.fun, R=400)
jack.after.boot(bout)
```

Let's go to R.

## Confidence Intervals

Standard errors are often used to assign approximate confidence intervals to a parameter of interest.

For example, given a parameter estimate $\hat{\theta}$ and an estimated standard error $\hat{se}$, the usual 95% confidence interval for $\theta$ is

$$\hat{\theta} \pm 1.96 \cdot \hat{se}$$

This gives us a best guess for $\theta$ and how far in error that guess might be (_assuming a symmetric distribution_).

## Symmetric versus Asymmetric Distributions

```{r, echo=FALSE}
library(boot)
set.seed(999)
x <- rnorm(10)
xbar1 <- exp(mean(x))
xbar2 <- log(exp(mean(x)))
xbar.fun <- function(x,i)exp(mean(x[i]))
xbar.fun2 <- function(x,i)log(exp(mean(x[i])))
bout1 <- boot(x,xbar.fun,R=1000)
bout2 <- boot(x,xbar.fun2,R=1000)
se1 <- sd(bout1$t)
se2 <- sd(bout2$t)
par(mfrow=c(1,2))
hist(bout1$t, main="Asymmetric (requires transformation)", xlab="", sub="Standard CI not accurate")
points(xbar1,0,col="red",pch=19)
abline(v=c(xbar1-1.96*se1, xbar1+1.96*se1),col="red")
hist(bout2$t, main="Symmetric (after transformation)", xlab="", sub="Standard CI works well")
points(xbar2,0,col="red",pch=19)
abline(v=c(xbar2-1.96*se2, xbar2+1.96*se2),col="red")
par(mfrow=c(1,1))

```


## Bootstrap Confidence Intervals

A goal of bootstrap methods is to produce dependable confidence intervals regardless of the distribution shape.

- match classic confidence intervals when such intervals are appropriate
- provide accurate coverage without transforming data

We'll look at two types:

1. Percentile
2. $BC_{a}$

## The Percentile Interval

How it works: use percentiles of the bootstrap histogram to define confidence limits.

Example: For a 95% percentile interval, the lower bound is the 2.5% percentile and the upper bound the 97.5% percentile.

If the bootstrap distribution is roughly normal, then the classic (standard normal) and percentile intervals will nearly agree.

The percentile method provides good coverage for asymmetric distributions and doesn't require transforming data.

```
# R code
boot.ci(boot.out, conf = 0.95, type = "perc")
```

## The Percentile Interval - Example


```{r, echo=FALSE}
pci <- boot.ci(bout1,type="perc")$percent
hist(bout1$t, main="Asymmetric bootstrap histogram", xlab="", sub="red lines = classic CI interval; blue lines = percentile interval")
points(xbar1,0,col="red",pch=19)
abline(v=c(xbar1-1.96*se1, xbar1+1.96*se1),col="red")
abline(v=c(pci[4],pci[5]),col="blue")
```

## The $BC_{a}$ Interval

An improved version of the percentile method. Stands for _Bias-corrected and accelerated_. 

Provides better coverage than the percentile method, automatically corrects for bias in the point estimate, and is preferred in practice.

More difficult to explain than the percentile interval, but not much more difficult to calculate.

```
# R code
boot.ci(boot.out, conf = 0.95, type = "bca")
```


## The $BC_{a}$ Interval - Example
```{r, echo=FALSE}
bcaci <- boot.ci(bout1,type="bca")$bca
hist(bout1$t, main="Asymmetric bootstrap histogram", xlab="", sub="red lines = classic CI interval; blue lines = percentile interval; green lines = bca interval")
points(xbar1,0,col="red",pch=19)
abline(v=c(xbar1-1.96*se1, xbar1+1.96*se1),col="red")
abline(v=c(pci[4],pci[5]),col="blue")
abline(v=c(bcaci[4],bcaci[5]),col="green")

```

## The $BC_{a}$ method

The $BC_{a}$ interval endpoints are also given by percentiles of the bootstrap distribution, but the percentiles are computed using a sophisticated method.

$$BC_{a}: (\hat{\theta}^{*(\alpha_{1})}, \hat{\theta}^{*(\alpha_{2})})$$
where
$$\alpha_{1} = \Phi(\hat{z}_{0} + \frac{\hat{z}_{0} + z^{\alpha}}{1-\hat{a}(\hat{z}_{0}+z^{\alpha})})$$

$\alpha_{2}$ is same as $\alpha_{1}$ except $1-\alpha$ replaces $\alpha$.

$\hat{a}$ is the acceleration and $\hat{z}_{0}$ is the bias-correction.


## The $BC_{a}$ method - continued

- Acceleration refers to the rate of change of the SE of $\hat{\theta}$ with respect to $\theta$.

- Bias refers to proportion of bootstrap replications less than $\hat{\theta}$.

The formulas for the acceleration and the bias-correction are complicated. See Efron, B. (1987) Better bootstrap confidence intervals (with Discussion). _Journal of the American Statistical Association_, **82**, 171-200.

Let's go to R.


## Bootstrapping Regressions

We can bootstrap standard errors of regression coefficients. Two ways:

1. *Case resampling*: select R bootstrap samples of the data (i.e. the rows) and fit a model to each sample. 
2. *Residual resampling*: fit a model to original data set and select R bootstrap samples of residuals to create new responses. We resample residuals and add to fitted reponse values matched to original corresponding observed predictors.

Bootstrapping regressions makes sense for models fit with methods other than least-squares.

## Example of Case Resampling

```{r}
# cars: speed of cars and the distances taken to stop
head(cars, n=4)
s <- sample(nrow(cars), replace=TRUE)
cars[s[1:4],]
```

## Example of Residual Resampling

```{r}
mod <- lm(dist ~ speed, data = cars) # fit model
rr  <- resid(mod)[s] # resample residuals
cbind(cars$speed[1:4],fitted(mod)[1:4] + rr[1:4])
```

## Case Resampling vs. Residual Resampling

If predictors are fixed (not random) values, may make more sense to use residual resampling.

Residual resampling enforces the strong assumption that errors are identically distributed. 

Case resampling is less sensitive to assumptions, therefore it may be preferable in some cases.

The car package provides a handy function for bootstrapping regression models: `Boot`

`Boot(model, R=1000, method="case")`  
`Boot(model, R=1000, method="residual")`  

Let's go to R.


## Cross-Validation

Cross-validation is a resampling method for estimating prediction error. 

Prediction error measures how well a model works.

Assessing this with data used to build model leads to underestimating error rate.

However we don't usually have new data to test the model.

To get around this, cross-validation uses part of the available data to fit the model and a different part to test it.


## How Cross-Validation Works

1. Randomly split data into $K$ roughly equal-sized parts. (Typically $K = 5$ or $10$)

2. Hold out the _k_th part and fit the model with the other $K - 1$ parts.

3. Calculate the prediction error of the fitted model using the _k_th part.

4. Do the above for $k = 1,2,\dots K$ and combine the $K$ estimates of prediction error for regression and classification, respectively:

    + $CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k}MSE_{i}$ where $MSE_{i} = \frac{1}{n_{i}}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^2$ (Mean Square Error)
    + $CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k}Err_{i}$ where $Err_{i} = \frac{1}{n_{i}}\sum_{i=1}^{n}I(y_{i} \neq \hat{y}_{i})$ (Classification Error Rate)



## Cross-Validation Notes

When $K = n$ we call this "leave-one-out" cross-validation (LOOCV). 

Using $K = 5$ or $K = 10$ often gives more accurate estimates of prediction error and requires less computation.

The `boot` package provides the `cv.glm` function to calculate K-fold cross-validation prediction error, which requires fitting a model using `glm`.

Cross validation also works for more complex modeling methods.

Let's go to R.

## References

Canty, A.J. (2002) _Resampling methods in R: the boot package_. R News. 2/3, 2-7.

Davison, A.C. and Hinkley, D.V. (1997) _Bootstrap methods and their applications_. Cambridge University Press, Cambridge.

Davison, A.C. and Kuonen, D. (2002) _An introduction to the Bootstrap with applications in R_. Statistical Computing & Statistical Graphics Newsletter. 13, 6-11.

Diaconis, P. and Efron, B. (1983) _Computer-Intensive Methods in Statistics_. Scientific American. Vol 248, No 5, 116-130.

Efron, B. and Tibshirani, R.J. (1993) _An Introduction to the Bootstrap_. Chapman and Hall, London, New York.

## References (cont'd)

Fox, J. and Weisberg, S. (2012) _Bootstrapping regression models in R, an appendix to an R Companion to Applied Regression, 2nd ed_. Sage, London.

James, G. et al. (2013) _An Introduction to Statistical Learning_. Springer, New York.

Maindonald, J. and Braun, J.W. (2010) _Data Analysis and Graphics Using R, 3rd Ed_. Cambridge University Press, Cambridge.


## StatLab

Thanks for coming today!

For help and advice with your data analysis, contact the StatLab to set up an appointment: statlab@virginia.edu

Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/statlab/

Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/
